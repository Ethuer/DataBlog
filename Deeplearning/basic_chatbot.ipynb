{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on https://github.com/tensorlayer/seq2seq-chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import ast\n",
    "import os\n",
    "import nltk\n",
    "import gzip\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorlayer as tl\n",
    "\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from tensorlayer.cost import cross_entropy_seq, cross_entropy_seq_with_mask\n",
    "from tensorlayer.models.seq2seq import Seq2seq\n",
    "from tensorlayer.models.seq2seq_with_attention import Seq2seqLuongAttention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ethuer/anaconda3/envs/tf2/lib/python3.7/site-packages/urllib3/connectionpool.py:1004: InsecureRequestWarning: Unverified HTTPS request is being made to host 'jmcauley.ucsd.edu'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n"
     ]
    }
   ],
   "source": [
    "# get the data\n",
    "\n",
    "url = 'https://jmcauley.ucsd.edu/data/amazon/qa/qa_Electronics.json.gz'\n",
    "\n",
    "r = requests.get(url,verify=False)\n",
    "\n",
    "with open('qa_Electronics.json.gz','wb') as localfile:\n",
    "    localfile.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# the data is not in proper JSON format, so iterate over rows to get the dictionaries with ast\n",
    "data_dict = {}\n",
    "with gzip.open('qa_Electronics.json.gz','rb') as jsonfile:\n",
    "    for count, row in enumerate(jsonfile):\n",
    "        \n",
    "        row  = row.decode(\"utf-8\") \n",
    "        data_dict[count] = ast.literal_eval(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "\n",
    "data = pd.DataFrame.from_dict(data_dict, orient='index')\n",
    "\n",
    "vocabulary_size = 10000\n",
    "\n",
    "# test train split\n",
    "data = shuffle(data)\n",
    "test_train_split = 0.9\n",
    "\n",
    "train = data[:int(data.shape[0]*test_train_split)]\n",
    "test = data[int(data.shape[0]*test_train_split):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ' '.join(train.question.tolist() + train.answer.tolist() )\n",
    "word_dict = Counter(nltk.word_tokenize(sentences) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_df = pd.DataFrame.from_dict({\"frequency\":word_dict}, orient='columns')\n",
    "word_df = word_df.sort_values('frequency', ascending=False)[:vocabulary_size]\n",
    "\n",
    "# add ranking\n",
    "word_df['rank'] = word_df.reset_index().index +4\n",
    "\n",
    "# add word column\n",
    "word_df['word'] = word_df.index\n",
    "\n",
    "# create mapping dictionary\n",
    "idx2word = word_df.set_index('rank')['word'].to_dict()\n",
    "word2idx = word_df.set_index('word')['rank'].to_dict()\n",
    "\n",
    "pad_id = 0\n",
    "unk_id = 1\n",
    "start_id = 2\n",
    "end_id = 3\n",
    "\n",
    "word2idx['<pad>'] = pad_id\n",
    "idx2word[pad_id] = '<pad>'\n",
    "\n",
    "word2idx['<unk>'] = unk_id\n",
    "idx2word[unk_id] = '<unk>'\n",
    "\n",
    "word2idx['<start>'] = start_id\n",
    "idx2word[start_id] = '<start>'\n",
    "\n",
    "word2idx['<end>'] = end_id\n",
    "idx2word[end_id] = '<end>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sentence2idx(sentence):\n",
    "    \"\"\"\n",
    "    create integer list from sentence\n",
    "    \"\"\"\n",
    "    \n",
    "    outsentence = []\n",
    "    for word in nltk.word_tokenize(sentence):\n",
    "        if word in word2idx:\n",
    "            outsentence.append(word2idx[word])\n",
    "        else:\n",
    "            outsentence.append(word2idx['<unk>'])\n",
    "    \n",
    "    return outsentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainX = [sentence2idx(sent) for sent in train.question.tolist() ]\n",
    "trainY = [sentence2idx(sent) for sent in train.answer.tolist() ]\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "n_step = len(trainX) // batch_size\n",
    "src_vocab_size = len(word2idx)\n",
    "emb_dim = 300\n",
    "\n",
    "#word2idx = metadata['w2idx']   # dict  word 2 index\n",
    "#idx2word = metadata['idx2w']   # list index 2 word\n",
    "\n",
    "\n",
    "src_vocab_size = tgt_vocab_size = len(word2idx) + 2\n",
    "\n",
    "num_epochs = 2\n",
    "vocabulary_size = src_vocab_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "\n",
    "def inference(seed, top_n):\n",
    "        model_.eval()\n",
    "        seed_id = [word2idx.get(w, unk_id) for w in seed.split(\" \")]\n",
    "        sentence_int = model_(inputs=[[seed_id]], seq_length=20, start_token=start_id, top_n = top_n)\n",
    "        sentence_int = sentence_int.numpy().tolist()[0]\n",
    "        \n",
    "        sentence = []\n",
    "        for w_int in sentence_int:\n",
    "            word = idx2word[w_int]\n",
    "            if word == 'end_id':\n",
    "                break\n",
    "            sentence = sentence + [word]\n",
    "        return sentence\n",
    "\n",
    "decoder_seq_length = 40\n",
    "model_ = Seq2seq(\n",
    "        decoder_seq_length = decoder_seq_length,\n",
    "        cell_enc=tf.keras.layers.GRUCell,\n",
    "        cell_dec=tf.keras.layers.GRUCell,\n",
    "        n_layer=3,\n",
    "        n_units=256,\n",
    "        embedding_layer=tl.layers.Embedding(vocabulary_size=vocabulary_size, embedding_size=emb_dim),\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = test.sample(3).question.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "        model_.train()\n",
    "        \n",
    "        # shuffle trainingsdata\n",
    "        trainX, trainY = shuffle(trainX, trainY, random_state=0)\n",
    "        \n",
    "        # reset loss\n",
    "        total_loss, n_iter = 0, 0\n",
    "        for X, Y in tqdm(tl.iterate.minibatches(inputs=trainX, targets=trainY, batch_size=batch_size, shuffle=False), \n",
    "                        total=n_step, desc='Epoch[{}/{}]'.format(epoch + 1, num_epochs), leave=False):\n",
    "\n",
    "            X = tl.prepro.pad_sequences(X)\n",
    "            _target_seqs = tl.prepro.sequences_add_end_id(Y, end_id=end_id)\n",
    "            _target_seqs = tl.prepro.pad_sequences(_target_seqs, maxlen=decoder_seq_length)\n",
    "            _decode_seqs = tl.prepro.sequences_add_start_id(Y, start_id=start_id, remove_last=False)\n",
    "            _decode_seqs = tl.prepro.pad_sequences(_decode_seqs, maxlen=decoder_seq_length)\n",
    "            _target_mask = tl.prepro.sequences_get_mask(_target_seqs)\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                ## compute outputs\n",
    "                output = model_(inputs = [X, _decode_seqs])\n",
    "                \n",
    "                output = tf.reshape(output, [-1, vocabulary_size])\n",
    "                \n",
    "                ## compute loss and update model\n",
    "                loss = cross_entropy_seq_with_mask(logits=output, target_seqs=_target_seqs, input_mask=_target_mask)\n",
    "\n",
    "                grad = tape.gradient(loss, model_.all_weights)\n",
    "                optimizer.apply_gradients(zip(grad, model_.all_weights))\n",
    "            \n",
    "            total_loss += loss\n",
    "            n_iter += 1\n",
    "\n",
    "        # printing average loss after every epoch\n",
    "        print('Epoch [{}/{}]: loss {:.4f}'.format(epoch + 1, num_epochs, total_loss / n_iter))\n",
    "\n",
    "        for seed in seeds:\n",
    "            print(\"Query >\", seed)\n",
    "            top_n = 3\n",
    "            for i in range(top_n):\n",
    "                sentence = inference(seed, top_n)\n",
    "                print(\" >\", ' '.join(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(seed, top_n):\n",
    "        model_.eval()\n",
    "        seed_id = [word2idx.get(w, unk_id) for w in seed.split(\" \")]\n",
    "        sentence_int = model_(inputs=[[seed_id]], seq_length=20, start_token=start_id, top_n = top_n)\n",
    "        sentence_int = sentence_int.numpy().tolist()[0]\n",
    "        sentence = []\n",
    "        for w_int in sentence_int:\n",
    "            if w_int == end_id:\n",
    "                break\n",
    "            word = idx2word[w_int]\n",
    "            sentence = sentence + [word]\n",
    "        return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30 minute Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qustion ':' Does this remote also replace NB677\n",
      "Answer 1 : Yes it will .\n",
      "Answer 2 : Yes it does\n",
      "Answer 3 : Yes\n",
      "Qustion ':' does it convert to metric measurement\n",
      "Answer 1 : Yes , it does . It is very nice to be . <unk>\n",
      "Answer 2 : Yes , it does not come with a <unk> .\n",
      "Answer 3 : Yes\n",
      "Qustion ':' What comes with the camera, buying it here?\n",
      "Answer 1 : I have a <unk> <unk> <unk> . It is a <unk> <unk> and the <unk> <unk> is a little <unk>\n",
      "Answer 2 : I have no idea that I know I bought the same one . It 's a little . I would\n",
      "Answer 3 : I bought a question . I bought it for a few months and the same thing is <unk>\n"
     ]
    }
   ],
   "source": [
    "for seed in seeds:\n",
    "            print(\"Question  :\", seed)\n",
    "            top_n = 3\n",
    "            for i in range(top_n):\n",
    "                sentence = inference(seed, top_n)\n",
    "                print(f\"Answer {i+1} :\", ' '.join(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qustion ':' Does this remote also replace NB677\n",
      "Answer 1 : <unk> . Good <unk> ! ! ! ! <unk> . Thanks ! <unk>\n",
      "Answer 2 : <unk> .\n",
      "Answer 3 : <unk> , it will work\n",
      "Qustion ':' does it convert to metric measurement\n",
      "Answer 1 : <unk> .\n",
      "Answer 2 : <unk> , <unk> ,\n",
      "Answer 3 : <unk> , <unk> ,\n",
      "Qustion ':' What comes with the camera, buying it here?\n",
      "Answer 1 : <unk> <unk>\n",
      "Answer 2 : <unk> <unk>\n",
      "Answer 3 : <unk> <unk> . <unk>\n"
     ]
    }
   ],
   "source": [
    "# ~ 1 hour Training\n",
    "for seed in seeds:\n",
    "            print(\"Question :\", seed)\n",
    "            top_n = 3\n",
    "            for i in range(top_n):\n",
    "                sentence = inference(seed, top_n)\n",
    "                print(f\"Answer {i+1} :\", ' '.join(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
