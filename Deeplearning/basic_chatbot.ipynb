{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import ast\n",
    "import os\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorlayer as tl\n",
    "\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from tensorlayer.cost import cross_entropy_seq, cross_entropy_seq_with_mask\n",
    "from tensorlayer.models.seq2seq import Seq2seq\n",
    "from tensorlayer.models.seq2seq_with_attention import Seq2seqLuongAttention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {}\n",
    "with open('/home/ethuer/Blog/blogposts/Deeplearning/Data/qa_Electronics.json','r') as jsonfile:\n",
    "    for count, row in enumerate(jsonfile):\n",
    "        \n",
    "        data_dict[count] = ast.literal_eval(row)\n",
    "        \n",
    "\n",
    "data = pd.DataFrame.from_dict(data_dict, orient='index')\n",
    "\n",
    "vocabulary_size = 8000\n",
    "\n",
    "# test train split\n",
    "data = shuffle(data)\n",
    "test_train_split = 0.8\n",
    "\n",
    "train = data[:int(data.shape[0]*test_train_split)]\n",
    "test = data[int(data.shape[0]*test_train_split):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ' '.join(train.question.tolist() + train.answer.tolist() )\n",
    "word_dict = Counter(nltk.word_tokenize(sentences) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_df = pd.DataFrame.from_dict({\"frequency\":word_dict}, orient='columns')\n",
    "word_df = word_df.sort_values('frequency', ascending=False)[:vocabulary_size]\n",
    "\n",
    "# add ranking\n",
    "word_df['rank'] = word_df.reset_index().index +4\n",
    "\n",
    "# add word column\n",
    "word_df['word'] = word_df.index\n",
    "\n",
    "# create mapping dictionary\n",
    "idx2word = word_df.set_index('rank')['word'].to_dict()\n",
    "word2idx = word_df.set_index('word')['rank'].to_dict()\n",
    "\n",
    "pad_id = 0\n",
    "unk_id = 1\n",
    "end_id = 2\n",
    "stop_id = 3\n",
    "\n",
    "word2idx['<pad>'] = pad_id\n",
    "idx2word[pad_id] = '<pad>'\n",
    "\n",
    "word2idx['<unk>'] = unk_id\n",
    "idx2word[unk_id] = '<unk>'\n",
    "\n",
    "word2idx['<start>'] = start_id\n",
    "idx2word[start_id] = '<start>'\n",
    "\n",
    "word2idx['<end>'] = end_id\n",
    "idx2word[end_id] = '<end>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def sentence2idx(sentence):\n",
    "    \"\"\"\n",
    "    create integer list from sentence\n",
    "    \"\"\"\n",
    "    \n",
    "    outsentence = []\n",
    "    for word in nltk.word_tokenize(sentence):\n",
    "        if word in word2idx:\n",
    "            outsentence.append(word2idx[word])\n",
    "        else:\n",
    "            outsentence.append(word2idx['<unk>'])\n",
    "    \n",
    "    return outsentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainX = [sentence2idx(sent) for sent in train.question.tolist() ]\n",
    "trainY = [sentence2idx(sent) for sent in train.answer.tolist() ]\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "n_step = len(trainX) // batch_size\n",
    "src_vocab_size = len(word2idx)\n",
    "emb_dim = 300\n",
    "\n",
    "#word2idx = metadata['w2idx']   # dict  word 2 index\n",
    "#idx2word = metadata['idx2w']   # list index 2 word\n",
    "\n",
    "\n",
    "src_vocab_size = tgt_vocab_size = len(word2idx) + 2\n",
    "\n",
    "num_epochs = 10\n",
    "vocabulary_size = src_vocab_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TL] Embedding embedding_4: (8006, 300)\n",
      "[TL] RNN rnn_13: cell: GRUCell, n_units: 256\n",
      "[TL] RNN rnn_14: cell: GRUCell, n_units: 256\n",
      "[TL] RNN rnn_15: cell: GRUCell, n_units: 256\n",
      "[TL] RNN rnn_16: cell: GRUCell, n_units: 256\n",
      "[TL] RNN rnn_17: cell: GRUCell, n_units: 256\n",
      "[TL] RNN rnn_18: cell: GRUCell, n_units: 256\n",
      "[TL] Reshape reshape_7\n",
      "[TL] Dense  dense_3: 8006 No Activation\n",
      "[TL] Reshape reshape_8\n",
      "[TL] Reshape reshape_9\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "\n",
    "def inference(seed, top_n):\n",
    "        model_.eval()\n",
    "        seed_id = [word2idx.get(w, unk_id) for w in seed.split(\" \")]\n",
    "        sentence_id = model_(inputs=[[seed_id]], seq_length=20, start_token=start_id, top_n = top_n)\n",
    "        sentence_id = sentence_id.numpy().tolist()[0]\n",
    "        sentence = []\n",
    "        for w_id in sentence_id:\n",
    "            w = idx2word[w_id]\n",
    "            if w == 'end_id':\n",
    "                break\n",
    "            sentence = sentence + [w]\n",
    "        return sentence\n",
    "\n",
    "decoder_seq_length = 20\n",
    "model_ = Seq2seq(\n",
    "        decoder_seq_length = decoder_seq_length,\n",
    "        cell_enc=tf.keras.layers.GRUCell,\n",
    "        cell_dec=tf.keras.layers.GRUCell,\n",
    "        n_layer=3,\n",
    "        n_units=256,\n",
    "        embedding_layer=tl.layers.Embedding(vocabulary_size=vocabulary_size, embedding_size=emb_dim),\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = test.sample(3).question.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Is it Bluetooth compatible?',\n",
       " 'Is this a new product?',\n",
       " 'is this work with canon rebel t3?']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[1/10]:   1%|          | 91/7856 [00:27<48:23,  2.67it/s]"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "        model_.train()\n",
    "        \n",
    "        # shuffle trainingsdata\n",
    "        trainX, trainY = shuffle(trainX, trainY, random_state=0)\n",
    "        \n",
    "        # reset loss\n",
    "        total_loss, n_iter = 0, 0\n",
    "        for X, Y in tqdm(tl.iterate.minibatches(inputs=trainX, targets=trainY, batch_size=batch_size, shuffle=False), \n",
    "                        total=n_step, desc='Epoch[{}/{}]'.format(epoch + 1, num_epochs), leave=False):\n",
    "\n",
    "            X = tl.prepro.pad_sequences(X)\n",
    "            _target_seqs = tl.prepro.sequences_add_end_id(Y, end_id=end_id)\n",
    "            _target_seqs = tl.prepro.pad_sequences(_target_seqs, maxlen=decoder_seq_length)\n",
    "            _decode_seqs = tl.prepro.sequences_add_start_id(Y, start_id=start_id, remove_last=False)\n",
    "            _decode_seqs = tl.prepro.pad_sequences(_decode_seqs, maxlen=decoder_seq_length)\n",
    "            _target_mask = tl.prepro.sequences_get_mask(_target_seqs)\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                ## compute outputs\n",
    "                output = model_(inputs = [X, _decode_seqs])\n",
    "                \n",
    "                output = tf.reshape(output, [-1, vocabulary_size])\n",
    "                ## compute loss and update model\n",
    "                loss = cross_entropy_seq_with_mask(logits=output, target_seqs=_target_seqs, input_mask=_target_mask)\n",
    "\n",
    "                grad = tape.gradient(loss, model_.all_weights)\n",
    "                optimizer.apply_gradients(zip(grad, model_.all_weights))\n",
    "            \n",
    "            total_loss += loss\n",
    "            n_iter += 1\n",
    "\n",
    "        # printing average loss after every epoch\n",
    "        print('Epoch [{}/{}]: loss {:.4f}'.format(epoch + 1, num_epochs, total_loss / n_iter))\n",
    "\n",
    "        for seed in seeds:\n",
    "            print(\"Query >\", seed)\n",
    "            top_n = 3\n",
    "            for i in range(top_n):\n",
    "                sentence = inference(seed, top_n)\n",
    "                print(\" >\", ' '.join(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
